{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPILib ML Training Notebook\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By using this notebook, you can train a TensorFlow Lite model for use on a Raspberry Pi and Google Coral USB Accelerator. We've designed this process to be as simple as possible. If you find an issue with this notebook, please create a new issue report on our [GitHub page](https://github.com/wpilibsuite/CoralSagemaker), where you downloaded this notebook.\n",
    "\n",
    "Complete instructions on how to train a model can be found [here](https://github.com/wpilibsuite/CoralSagemaker/blob/master/docs/training.md).\n",
    "\n",
    "The code below will take longer depending on your value for 'epochs'. A higher value will take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 21:22:13 Starting - Starting the training job...\n",
      "2020-01-01 21:22:14 Starting - Launching requested ML instances......\n",
      "2020-01-01 21:23:15 Starting - Preparing the instances for training......\n",
      "2020-01-01 21:24:41 Downloading - Downloading input data\n",
      "2020-01-01 21:24:41 Training - Downloading the training image..............\u001b[34m.\u001b[0m\n",
      "\n",
      "2020-01-01 21:26:48 Training - Training image download completed. Training in progress.\u001b[34mDownloading model.\u001b[0m\n",
      "\u001b[34mSuccessfully created the TFRecords: /opt/ml/input/data/training/train.record.\u001b[0m\n",
      "\u001b[34mSuccessfully created the TFRecords: /opt/ml/input/data/training/eval.record.\u001b[0m\n",
      "\u001b[34mRecords generated.\u001b[0m\n",
      "\u001b[34mHyperparameters parsed.\u001b[0m\n",
      "\u001b[34mBeginning training on Docker image\u001b[0m\n",
      "\u001b[34mResults of training:\n",
      "    Checkpoint 100 accuracy: 15.813%\n",
      "    Checkpoint 200 accuracy: 17.604%\n",
      "    Checkpoint 300 accuracy: 19.542%\n",
      "    Checkpoint 400 accuracy: 18.589%\n",
      "    Checkpoint 500 accuracy: 21.182%\n",
      "    Checkpoint 600 accuracy: 22.494%\n",
      "    Checkpoint 700 accuracy: 22.662%\n",
      "    Checkpoint 800 accuracy: 22.293%\n",
      "    Checkpoint 900 accuracy: 22.636%\n",
      "    Checkpoint 1000 accuracy: 23.069%\n",
      "\u001b[0m\n",
      "\u001b[34mCheckpoint 1000 will be converted..\u001b[0m\n",
      "\u001b[34mConverting checkpoint to tflite.\u001b[0m\n",
      "\u001b[34mCompiling model for Edge TPU\u001b[0m\n",
      "\n",
      "2020-01-01 21:47:58 Uploading - Uploading generated training model\n",
      "2020-01-01 21:47:58 Completed - Training job completed\n",
      "Training seconds: 1407\n",
      "Billable seconds: 1407\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Uses GPU by default, change to false to use CPU\n",
    "use_gpu = True\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "instance_type = None\n",
    "algorithm_name = None\n",
    "\n",
    "if use_gpu:\n",
    "    instance_type = 'ml.p3.2xlarge'\n",
    "    algorithm_name = 'wpi-gpu'\n",
    "else:\n",
    "    instance_type = 'ml.c4.2xlarge'\n",
    "    algorithm_name = 'wpi-cpu'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hyperparameters:\n",
    "    epochs -> int: number of training steps. Training time is proportional to this number. default = 700\n",
    "    batch_size -> int: size of a batch of training images. default = 32\n",
    "\"\"\"\n",
    "hyperparameters = {'epochs': 1000,\n",
    "                   'batch_size': 32}\n",
    "\n",
    "ecr_image = \"249838237784.dkr.ecr.us-east-1.amazonaws.com/{}:latest\".format(algorithm_name)\n",
    "\n",
    "# The estimator object, using our notebook, training instance, the ECR image, and the specified training steps\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\n",
    "# s3://wpilib\n",
    "estimator.fit(\"s3://wpilib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "You can download your trained model after the above step tells you \"Training job completed\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
